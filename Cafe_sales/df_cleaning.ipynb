{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b56431",
   "metadata": {},
   "source": [
    "# Cleaning: Dirty Cafe Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c3f1a",
   "metadata": {},
   "source": [
    "This notebook focuses exclusively on data preparation and cleaning. The input is a dataset containing transaction data from a cafÃ©, which shows signs of contamination (missing values, inconsistent formats, text errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe46f71",
   "metadata": {},
   "source": [
    "### ðŸ“‚ Dataset Info\n",
    "* **Source:** [Kaggle - Dirty Cafe Sales Dataset](https://www.kaggle.com/datasets/ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training)\n",
    "* **File:** `dirty_cafe_sales.csv`\n",
    "* **Description:** Synthetic dirty data created specifically for cleaning practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461955b4",
   "metadata": {},
   "source": [
    "## Loading data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "50a84adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset: 10000 rows, 8 columns\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dirty_cafe_sales.csv\")\n",
    "\n",
    "# Number of rows and columns\n",
    "print(f\"Loaded Dataset: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ea9d1",
   "metadata": {},
   "source": [
    "## Basic data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3fe71e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Item</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price Per Unit</th>\n",
       "      <th>Total Spent</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Location</th>\n",
       "      <th>Transaction Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_1961373</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_4977031</td>\n",
       "      <td>Cake</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_4271903</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_7034554</td>\n",
       "      <td>Salad</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_3160411</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Transaction ID    Item Quantity Price Per Unit Total Spent  Payment Method  \\\n",
       "0    TXN_1961373  Coffee        2            2.0         4.0     Credit Card   \n",
       "1    TXN_4977031    Cake        4            3.0        12.0            Cash   \n",
       "2    TXN_4271903  Cookie        4            1.0       ERROR     Credit Card   \n",
       "3    TXN_7034554   Salad        2            5.0        10.0         UNKNOWN   \n",
       "4    TXN_3160411  Coffee        2            2.0         4.0  Digital Wallet   \n",
       "\n",
       "   Location Transaction Date  \n",
       "0  Takeaway       2023-09-08  \n",
       "1  In-store       2023-05-16  \n",
       "2  In-store       2023-07-19  \n",
       "3   UNKNOWN       2023-04-27  \n",
       "4  In-store       2023-06-11  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8542eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transaction ID', 'Item', 'Quantity', 'Price Per Unit', 'Total Spent', 'Payment Method', 'Location', 'Transaction Date']\n",
      "['transaction_id', 'item', 'quantity', 'price_per_unit', 'total_spent', 'payment_method', 'location', 'transaction_date']\n"
     ]
    }
   ],
   "source": [
    "# Printing names of columns\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Renaming columns for consistency\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Checking the correctness of names\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dd7bff71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   transaction_id    10000 non-null  object\n",
      " 1   item              9667 non-null   object\n",
      " 2   quantity          9862 non-null   object\n",
      " 3   price_per_unit    9821 non-null   object\n",
      " 4   total_spent       9827 non-null   object\n",
      " 5   payment_method    7421 non-null   object\n",
      " 6   location          6735 non-null   object\n",
      " 7   transaction_date  9841 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bed23f",
   "metadata": {},
   "source": [
    "ðŸš© **Initial Findings**\n",
    "Based on the `df.info()` output, immediate cleaning is required for data types:\n",
    "\n",
    "* **Numeric Columns (`Quantity`, `Price Per Unit`, `Total Spent`):** Currently stored as `object` (string) instead of numbers, likely due to non-numeric values.\n",
    "\n",
    "* **Date Column (`Transaction Date`):** Currently stored as `object`. Needs conversion to `datetime` format for time-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c20534",
   "metadata": {},
   "source": [
    "### Duplicates and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24132990",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2db7c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicated rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305567dd",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d9fb8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: \n",
      " transaction_id         0\n",
      "item                 333\n",
      "quantity             138\n",
      "price_per_unit       179\n",
      "total_spent          173\n",
      "payment_method      2579\n",
      "location            3265\n",
      "transaction_date     159\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of missing values: \\n {df.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56c59a",
   "metadata": {},
   "source": [
    "ðŸ“‰ Missing Values Summary\n",
    "\n",
    "The output above reveals significant gaps in the dataset that require immediate attention:\n",
    "\n",
    "* **High Severity:** The `Location` (3265 missing) and `Payment Method` (2579 missing) columns are heavily compromised, missing approximately **25-33%** of data. Dropping these rows would result in massive data loss.\n",
    "\n",
    "* **Moderate Severity:** Essential operational columns like `Item`, `Price Per Unit`, and `Quantity` are missing hundreds of values. Since these are required for calculating total sales, we cannot simply ignore them.\n",
    "\n",
    "> **Strategy Required:** Simple removal (`dropna`) is not a viable option for columns like `Location` as we would lose too much data. We will need to combine **imputation** (filling gaps with \"Unknown\" or statistical averages) with **row removal** for critical missing operational data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb8a95",
   "metadata": {},
   "source": [
    "## Data Cleaning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332714c",
   "metadata": {},
   "source": [
    "### `Quantity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba367f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: int32\n",
      "------------------------------\n",
      "count    10000.00000\n",
      "mean         2.93130\n",
      "std          1.45079\n",
      "min          1.00000\n",
      "25%          2.00000\n",
      "50%          3.00000\n",
      "75%          4.00000\n",
      "max          5.00000\n",
      "Name: quantity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Standardize text format\n",
    "df['quantity'] = df['quantity'].astype(str).str.strip().str.title()\n",
    "\n",
    "# Force convert to numeric\n",
    "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
    "\n",
    "# Handle Missing Values \n",
    "# Assumption: A transaction implies at least 1 item was sold.\n",
    "df['quantity'] = df['quantity'].fillna(1)\n",
    "\n",
    "# Convert to Integer\n",
    "df['quantity'] = df['quantity'].astype(int)\n",
    "\n",
    "# 5. Verification\n",
    "print(f\"Data type: {df['quantity'].dtype}\")\n",
    "print(\"-\" * 30)\n",
    "print(df['quantity'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330ac06",
   "metadata": {},
   "source": [
    "### `Price` and `Total Spent` \n",
    "Both `Price Per Unit` and `Total Spent` columns share identical issues with the `Quantity` column: mixed types, string errors (\"ERROR\", \"UNKNOWN\"), and missing values.\n",
    "\n",
    "**Approach:**\n",
    "To adhere to the **DRY (Don't Repeat Yourself)** principle, we will define a reusable helper function `clean_float_column`.\n",
    "\n",
    "**Logic:**\n",
    "1.  **Coerce to Numeric:** Convert column to numbers; non-numeric strings become `NaN`.\n",
    "2.  **Imputation:** Fill missing values (`NaN`) with the **median** of the column. Using the median is robust against outliers (unlike the mean).\n",
    "3.  **Return:** A clean column of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types after cleaning:\n",
      "price_per_unit    float64\n",
      "total_spent       float64\n",
      "dtype: object\n",
      "------------------------------\n",
      "Preview of cleaned values:\n",
      "   price_per_unit  total_spent\n",
      "0             2.0          4.0\n",
      "1             3.0         12.0\n",
      "2             1.0          8.0\n",
      "3             5.0         10.0\n",
      "4             2.0          4.0\n"
     ]
    }
   ],
   "source": [
    "# Defining Cleaning Function\n",
    "\n",
    "def clean_float_column(series):\n",
    "    \"\"\"\n",
    "    Cleans a pandas Series by converting to numeric and filling NaNs with the median.\n",
    "    \"\"\"\n",
    "    # Force convert to numeric (errors -> NaN)\n",
    "    cleaned_series = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Calculate Median (ignoring NaNs automatically)\n",
    "    median_val = cleaned_series.median()\n",
    "    \n",
    "    # Fill Missing Values\n",
    "    cleaned_series = cleaned_series.fillna(median_val)\n",
    "    \n",
    "    return cleaned_series\n",
    "\n",
    "# Applying Function\n",
    "\n",
    "# Apply to 'Price Per Unit'\n",
    "df['price_per_unit'] = clean_float_column(df['price_per_unit'])\n",
    "\n",
    "# Apply to 'Total Spent'\n",
    "df['total_spent'] = clean_float_column(df['total_spent'])\n",
    "\n",
    "# Verification\n",
    "print(\"Data Types after cleaning:\")\n",
    "print(df[['price_per_unit', 'total_spent']].dtypes)\n",
    "print(\"-\" * 30)\n",
    "print(\"Preview of cleaned values:\")\n",
    "print(df[['price_per_unit', 'total_spent']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9309a",
   "metadata": {},
   "source": [
    "### `Item`, `Payment Method` & `Location`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e471b1",
   "metadata": {},
   "source": [
    "All three categorical columns (`Item`, `Payment Method`, `Location`) share identical issues: they contain inconsistencies like \"UNKNOWN\", \"ERROR\", missing values, and mixed casing.\n",
    "\n",
    "**Strategy:**\n",
    "We will define a reusable function `clean_categorical_column` that:\n",
    "1.  **Standardizes Text:** Converts everything to Title Case (e.g., \"latte\" -> \"Latte\", \"UNKNOWN\" -> \"Unknown\").\n",
    "2.  **Unifies Missing Data:** Maps all error placeholders (\"Error\", \"Nan\", empty strings) to a single category `'Unknown'`.\n",
    "\n",
    "This ensures consistency across all categorical variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "125ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Items: ['Coffee', 'Cake', 'Cookie', 'Salad', 'Smoothie', 'Unknown', 'Sandwich', 'Juice', 'Tea']\n",
      "------------------------------\n",
      "Unique Payment Methods: ['Credit Card', 'Cash', 'Unknown', 'Digital Wallet']\n",
      "------------------------------\n",
      "Unique Locations: ['Takeaway', 'In-Store', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# Defining Categorical Cleaning Function\n",
    "\n",
    "def clean_categorical_column(series):\n",
    "    \"\"\"\n",
    "    Cleans a categorical column by standardizing text case \n",
    "    and replacing error values with 'Unknown'.\n",
    "    \"\"\"\n",
    "    # Convert to string and standardise format \n",
    "    # This handles \"UNKNOWN\" -> \"Unknown\", \"nan\" -> \"Nan\", \"latte\" -> \"Latte\"\n",
    "    cleaned_series = series.astype(str).str.strip().str.title()\n",
    "    \n",
    "    # Define bad values to map to 'Unknown'\n",
    "    error_values = ['Error', 'Nan', '', 'Unknown'] \n",
    "    \n",
    "    # Replace bad values\n",
    "    cleaned_series = cleaned_series.replace(error_values, 'Unknown')\n",
    "    \n",
    "    return cleaned_series\n",
    "\n",
    "# Applying Function\n",
    "df['item'] = clean_categorical_column(df['item'])\n",
    "df['payment_method'] = clean_categorical_column(df['payment_method'])\n",
    "df['location'] = clean_categorical_column(df['location'])\n",
    "\n",
    "# Verification\n",
    "print(\"Unique Items:\", df['item'].unique().tolist())\n",
    "print(\"-\" * 30)\n",
    "print(\"Unique Payment Methods:\", df['payment_method'].unique().tolist())\n",
    "print(\"-\" * 30)\n",
    "print(\"Unique Locations:\", df['location'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a331f",
   "metadata": {},
   "source": [
    "### Transaction date\n",
    "The `Transaction Date` column contains invalid values like \"ERROR\" and \"UNKNOWN\", along with missing data. Since the date is a factual record critical for time-series analysis, it cannot be reliably imputed.\n",
    "\n",
    "**Strategy:**\n",
    "* **Method:** Drop rows.\n",
    "* **Justification:** Dropping approx. 1.6% of data is preferable to fabricating timestamps, which would distort sales trends and daily volume analysis.\n",
    "\n",
    "**Action:**\n",
    "1.  Convert the column to datetime objects using `errors='coerce'` (turns invalid text into `NaT`).\n",
    "2.  Remove all rows where the date is `NaT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c2c034d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid dates to be dropped: 460 rows (4.60%)\n",
      "------------------------------\n",
      "Final dataset shape: (9540, 8)\n",
      "Date column type: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Convert to Datetime\n",
    "# 'errors='coerce'' handles \"ERROR\", \"UNKNOWN\" by turning them into NaT (Not a Time)\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "\n",
    "# Assess Data Loss\n",
    "n_missing_dates = df['transaction_date'].isna().sum()\n",
    "print(f\"Invalid dates to be dropped: {n_missing_dates} rows ({n_missing_dates/len(df):.2%})\")\n",
    "\n",
    "# Drop rows\n",
    "df = df.dropna(subset=['transaction_date'])\n",
    "\n",
    "# Verification\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Date column type: {df['transaction_date'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffef6ec",
   "metadata": {},
   "source": [
    "## Final Validation & Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4454c0a",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c1637a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9540 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   transaction_id    9540 non-null   object        \n",
      " 1   item              9540 non-null   object        \n",
      " 2   quantity          9540 non-null   int32         \n",
      " 3   price_per_unit    9540 non-null   float64       \n",
      " 4   total_spent       9540 non-null   float64       \n",
      " 5   payment_method    9540 non-null   object        \n",
      " 6   location          9540 non-null   object        \n",
      " 7   transaction_date  9540 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(2), int32(1), object(4)\n",
      "memory usage: 633.5+ KB\n",
      "------------------------------\n",
      "Remaining Missing Values: \n",
      "\n",
      "transaction_id      0\n",
      "item                0\n",
      "quantity            0\n",
      "price_per_unit      0\n",
      "total_spent         0\n",
      "payment_method      0\n",
      "location            0\n",
      "transaction_date    0\n",
      "dtype: int64\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check Data Types & Non-Null Counts\n",
    "# We expect: no 'object' types for numbers/dates, and 0 missing values.\n",
    "print(\"Data Info: \\n\")\n",
    "df.info()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Verify No Missing Values\n",
    "# Should be all zeros.\n",
    "print(\"Remaining Missing Values: \\n\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "21c3c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary: \n",
      "\n",
      "          quantity  price_per_unit  total_spent               transaction_date\n",
      "count  9540.000000     9540.000000  9540.000000                           9540\n",
      "mean      2.928931        2.952673     8.868816  2023-07-01 23:00:31.698113536\n",
      "min       1.000000        1.000000     1.000000            2023-01-01 00:00:00\n",
      "25%       2.000000        2.000000     4.000000            2023-04-01 00:00:00\n",
      "50%       3.000000        3.000000     8.000000            2023-07-02 00:00:00\n",
      "75%       4.000000        4.000000    12.000000            2023-10-02 00:00:00\n",
      "max       5.000000        5.000000    25.000000            2023-12-31 00:00:00\n",
      "std       1.449786        1.243449     5.861817                            NaN\n"
     ]
    }
   ],
   "source": [
    "# Statistical Summary\n",
    "# Check for anomalies (e.g., min quantity < 1, negative prices)\n",
    "print(\"Statistical Summary: \\n\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744df660",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e303dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! Cleaned data saved to 'clean_cafe_sales.csv'\n",
      "Final dataset shape: (9540, 8)\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "output_filename = 'clean_cafe_sales.csv'\n",
    "\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"âœ… Success! Cleaned data saved to '{output_filename}'\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
